{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting net.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile net.py\n",
    "import tensorflow as tf\n",
    "import util, data_gen\n",
    "import numpy as np\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "DISP_SCALING = 10\n",
    "MIN_DISP = 0.01\n",
    "WEIGHT_REG = 0.0005\n",
    "EGOMOTION_VEC_SIZE = 6\n",
    "\n",
    "    \n",
    "def _resize_like(inputs, ref):\n",
    "    if tf.shape(inputs)[1] == tf.shape(ref)[1] and tf.shape(inputs)[2] == tf.shape(ref)[2]:\n",
    "        return inputs\n",
    "    else:\n",
    "        return tf.image.resize_nearest_neighbor(inputs, [tf.shape(ref)[1], tf.shape(ref)[2]])    \n",
    "    \n",
    "def disp_net(target_image):\n",
    "    h = target_image.get_shape()[1].value\n",
    "    w = target_image.get_shape()[2].value\n",
    "    tf.shape(target_image)[1]\n",
    "    inputs = target_image\n",
    "    with tf.variable_scope('depth') as sc:\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        normalizer_fn = None\n",
    "        normalizer_params = None\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                                normalizer_fn=normalizer_fn,\n",
    "                                normalizer_params=normalizer_params,\n",
    "                                weights_regularizer=slim.l2_regularizer(WEIGHT_REG),\n",
    "                                activation_fn=tf.nn.relu):\n",
    "            cnv1 = slim.conv2d(target_image, 32, [7, 7], stride=2, scope='cnv1')\n",
    "            cnv1b = slim.conv2d(cnv1, 32, [7, 7], stride=1, scope='cnv1b')\n",
    "            cnv2 = slim.conv2d(cnv1b, 64, [5, 5], stride=2, scope='cnv2')\n",
    "            cnv2b = slim.conv2d(cnv2, 64, [5, 5], stride=1, scope='cnv2b')\n",
    "\n",
    "            cnv3 = slim.conv2d(cnv2b, 128, [3, 3], stride=2, scope='cnv3')\n",
    "            cnv3b = slim.conv2d(cnv3, 128, [3, 3], stride=1, scope='cnv3b')\n",
    "            cnv4 = slim.conv2d(cnv3b, 256, [3, 3], stride=2, scope='cnv4')\n",
    "            cnv4b = slim.conv2d(cnv4, 256, [3, 3], stride=1, scope='cnv4b')\n",
    "            cnv5 = slim.conv2d(cnv4b, 512, [3, 3], stride=2, scope='cnv5')\n",
    "            cnv5b = slim.conv2d(cnv5, 512, [3, 3], stride=1, scope='cnv5b')\n",
    "            cnv6 = slim.conv2d(cnv5b, 512, [3, 3], stride=2, scope='cnv6')\n",
    "            cnv6b = slim.conv2d(cnv6, 512, [3, 3], stride=1, scope='cnv6b')\n",
    "            cnv7 = slim.conv2d(cnv6b, 512, [3, 3], stride=2, scope='cnv7')\n",
    "            cnv7b = slim.conv2d(cnv7, 512, [3, 3], stride=1, scope='cnv7b')\n",
    "\n",
    "            up7 = slim.conv2d_transpose(cnv7b, 512, [3, 3], stride=2, scope='upcnv7')\n",
    "            up7 = _resize_like(up7, cnv6b)\n",
    "            i7_in = tf.concat([up7, cnv6b], axis=3)\n",
    "            icnv7 = slim.conv2d(i7_in, 512, [3, 3], stride=1, scope='icnv7')\n",
    "\n",
    "            up6 = slim.conv2d_transpose(icnv7, 512, [3, 3], stride=2, scope='upcnv6')\n",
    "            up6 = _resize_like(up6, cnv5b)\n",
    "            i6_in = tf.concat([up6, cnv5b], axis=3)\n",
    "            icnv6 = slim.conv2d(i6_in, 512, [3, 3], stride=1, scope='icnv6')\n",
    "\n",
    "            up5 = slim.conv2d_transpose(icnv6, 256, [3, 3], stride=2, scope='upcnv5')\n",
    "            up5 = _resize_like(up5, cnv4b)\n",
    "            i5_in = tf.concat([up5, cnv4b], axis=3)\n",
    "            icnv5 = slim.conv2d(i5_in, 256, [3, 3], stride=1, scope='icnv5')\n",
    "\n",
    "            up4 = slim.conv2d_transpose(icnv5, 128, [3, 3], stride=2, scope='upcnv4')\n",
    "            i4_in = tf.concat([up4, cnv3b], axis=3)\n",
    "            icnv4 = slim.conv2d(i4_in, 128, [3, 3], stride=1, scope='icnv4')\n",
    "            disp4 = (slim.conv2d(icnv4, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn=None, scope='disp4')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "            disp4_up = tf.image.resize_bilinear(disp4, [np.int(h / 4), np.int(w / 4)])\n",
    "\n",
    "            up3 = slim.conv2d_transpose(icnv4, 64, [3, 3], stride=2, scope='upcnv3')\n",
    "            i3_in = tf.concat([up3, cnv2b, disp4_up], axis=3)\n",
    "            icnv3 = slim.conv2d(i3_in, 64, [3, 3], stride=1, scope='icnv3')\n",
    "            disp3 = (slim.conv2d(icnv3, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn=None, scope='disp3')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "            disp3_up = tf.image.resize_bilinear(disp3, [np.int(h / 2), np.int(w / 2)])\n",
    "\n",
    "            up2 = slim.conv2d_transpose(icnv3, 32, [3, 3], stride=2, scope='upcnv2')\n",
    "            i2_in = tf.concat([up2, cnv1b, disp3_up], axis=3)\n",
    "            icnv2 = slim.conv2d(i2_in, 32, [3, 3], stride=1, scope='icnv2')\n",
    "            disp2 = (slim.conv2d(icnv2, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn=None, scope='disp2')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "            disp2_up = tf.image.resize_bilinear(disp2, [h, w])\n",
    "\n",
    "            up1 = slim.conv2d_transpose(icnv2, 16, [3, 3], stride=2, scope='upcnv1')\n",
    "            i1_in = tf.concat([up1, disp2_up], axis=3)\n",
    "            icnv1 = slim.conv2d(i1_in, 16, [3, 3], stride=1, scope='icnv1')\n",
    "            disp1 = (slim.conv2d(icnv1, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn=None, scope='disp1')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "\n",
    "            return [disp1, disp2, disp3, disp4]\n",
    "\n",
    "\n",
    "\n",
    "def egomotion_net(image_stack):\n",
    "    with tf.variable_scope('egomotion') as sc:\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        normalizer_fn = None\n",
    "        normalizer_params = None\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                            normalizer_fn=normalizer_fn,\n",
    "                            weights_regularizer=slim.l2_regularizer(WEIGHT_REG),\n",
    "                            normalizer_params=normalizer_params,\n",
    "                            activation_fn=tf.nn.relu):\n",
    "            cnv1 = slim.conv2d(image_stack, 16, [7, 7], stride=2, scope='cnv1')\n",
    "            cnv2 = slim.conv2d(cnv1, 32, [5, 5], stride=2, scope='cnv2')\n",
    "            cnv3 = slim.conv2d(cnv2, 64, [3, 3], stride=2, scope='cnv3')\n",
    "            cnv4 = slim.conv2d(cnv3, 128, [3, 3], stride=2, scope='cnv4')\n",
    "            cnv5 = slim.conv2d(cnv4, 256, [3, 3], stride=2, scope='cnv5')\n",
    "\n",
    "            with tf.variable_scope('pose'):\n",
    "                cnv6 = slim.conv2d(cnv5, 256, [3, 3], stride=2, scope='cnv6')\n",
    "                cnv7 = slim.conv2d(cnv6, 256, [3, 3], stride=2, scope='cnv7')\n",
    "                pred_channels = EGOMOTION_VEC_SIZE\n",
    "                egomotion_pred = slim.conv2d(cnv7,\n",
    "                                             pred_channels,\n",
    "                                             [1, 1],\n",
    "                                             scope='pred',\n",
    "                                             stride=1,\n",
    "                                             normalizer_fn=None,\n",
    "                                             activation_fn=None)\n",
    "                egomotion_avg = tf.reduce_mean(egomotion_pred, [1, 2])\n",
    "                egomotion_final = 0.01 * tf.reshape(egomotion_avg, \n",
    "                                                    [-1, EGOMOTION_VEC_SIZE])\n",
    "\n",
    "                return egomotion_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting net.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile net.py\n",
    "import tensorflow as tf\n",
    "import util, data_gen\n",
    "import numpy as np\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "DISP_SCALING = 10\n",
    "MIN_DISP = 0.01\n",
    "WEIGHT_REG = 0.0005\n",
    "EGOMOTION_VEC_SIZE = 6\n",
    "\n",
    "    \n",
    "def _resize_like(inputs, ref):\n",
    "    if tf.shape(inputs)[1] == tf.shape(ref)[1] and tf.shape(inputs)[2] == tf.shape(ref)[2]:\n",
    "        return inputs\n",
    "    else:\n",
    "        return tf.image.resize_nearest_neighbor(inputs, [tf.shape(ref)[1], tf.shape(ref)[2]])    \n",
    "\n",
    "def disp_net(target_image, is_training = True):\n",
    "    # predict inverse of depth form a single images.\n",
    "    batch_norm_params = {'is_training' : is_training}\n",
    "    inputs = target_image\n",
    "    \n",
    "    with tf.variable_scope('depth_net') as sc:\n",
    "        normalizer_fn = slim.batch_norm\n",
    "        normalizer_params = batch_norm_params\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                            normalizer_fn=normalizer_fn,\n",
    "                            normalizer_params=normalizer_params,\n",
    "                            weights_regularizer=slim.l2_regularizer(WEIGHT_REG),\n",
    "                            activation_fn=tf.nn.relu):\n",
    "            cnv1 = slim.conv2d(inputs, 32, [7, 7], stride=2, scope='cnv1')\n",
    "            cnv1b = slim.conv2d(cnv1, 32, [7, 7], stride=1, scope='cnv1b')\n",
    "            cnv2 = slim.conv2d(cnv1b, 64, [5, 5], stride=2, scope='cnv2')\n",
    "            cnv2b = slim.conv2d(cnv2, 64, [5, 5], stride=1, scope='cnv2b')\n",
    "\n",
    "            cnv3 = slim.conv2d(cnv2b, 128, [3, 3], stride=2, scope='cnv3')\n",
    "            cnv3b = slim.conv2d(cnv3, 128, [3, 3], stride=1, scope='cnv3b')\n",
    "            cnv4 = slim.conv2d(cnv3b, 256, [3, 3], stride=2, scope='cnv4')\n",
    "            cnv4b = slim.conv2d(cnv4, 256, [3, 3], stride=1, scope='cnv4b')\n",
    "            cnv5 = slim.conv2d(cnv4b, 512, [3, 3], stride=2, scope='cnv5')\n",
    "            cnv5b = slim.conv2d(cnv5, 512, [3, 3], stride=1, scope='cnv5b')\n",
    "            cnv6 = slim.conv2d(cnv5b, 512, [3, 3], stride=2, scope='cnv6')\n",
    "            cnv6b = slim.conv2d(cnv6, 512, [3, 3], stride=1, scope='cnv6b')\n",
    "            cnv7 = slim.conv2d(cnv6b, 512, [3, 3], stride=2, scope='cnv7')\n",
    "            cnv7b = slim.conv2d(cnv7, 512, [3, 3], stride=1, scope='cnv7b')\n",
    "\n",
    "            up7 = slim.conv2d_transpose(cnv7b, 512, [3, 3], stride=2, scope='upcnv7')\n",
    "            # There might be dimension mismatch due to uneven down/up-sampling.\n",
    "            up7 = _resize_like(up7, cnv6b)\n",
    "            i7_in = tf.concat([up7, cnv6b], axis=3)\n",
    "            icnv7 = slim.conv2d(i7_in, 512, [3, 3], stride=1, scope='icnv7')\n",
    "\n",
    "            up6 = slim.conv2d_transpose(icnv7, 512, [3, 3], stride=2, scope='upcnv6')\n",
    "            up6 = _resize_like(up6, cnv5b)\n",
    "            i6_in = tf.concat([up6, cnv5b], axis=3)\n",
    "            icnv6 = slim.conv2d(i6_in, 512, [3, 3], stride=1, scope='icnv6')\n",
    "\n",
    "            up5 = slim.conv2d_transpose(icnv6, 256, [3, 3], stride=2, scope='upcnv5')\n",
    "            up5 = _resize_like(up5, cnv4b)\n",
    "            i5_in = tf.concat([up5, cnv4b], axis=3)\n",
    "            icnv5 = slim.conv2d(i5_in, 256, [3, 3], stride=1, scope='icnv5')\n",
    "\n",
    "            up4 = slim.conv2d_transpose(icnv5, 128, [3, 3], stride=2, scope='upcnv4')\n",
    "            i4_in = tf.concat([up4, cnv3b], axis=3)\n",
    "            icnv4 = slim.conv2d(i4_in, 128, [3, 3], stride=1, scope='icnv4')\n",
    "            disp4 = (slim.conv2d(icnv4, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn = None, scope='disp4')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "            disp4_up = tf.image.resize_bilinear(disp4, [np.int(tf.shape(target_image)[1] / 4), np.int(tf.shape(target_image)[2] / 4)])\n",
    "\n",
    "            up3 = slim.conv2d_transpose(icnv4, 64, [3, 3], stride=2, scope='upcnv3')\n",
    "            i3_in = tf.concat([up3, cnv2b, disp4_up], axis=3)\n",
    "            icnv3 = slim.conv2d(i3_in, 64, [3, 3], stride=1, scope='icnv3')\n",
    "            disp3 = (slim.conv2d(icnv3, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn = None, scope='disp3')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "            disp3_up = tf.image.resize_bilinear(disp3, [np.int(tf.shape(target_image)[1] / 2), np.int(tf.shape(target_image)[2] / 2)])\n",
    "\n",
    "            up2 = slim.conv2d_transpose(icnv3, 32, [3, 3], stride=2, scope='upcnv2')\n",
    "            i2_in = tf.concat([up2, cnv1b, disp3_up], axis=3)\n",
    "            icnv2 = slim.conv2d(i2_in, 32, [3, 3], stride=1, scope='icnv2')\n",
    "            disp2 = (slim.conv2d(icnv2, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn = None, scope='disp2')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "            disp2_up = tf.image.resize_bilinear(disp2, [tf.shape(target_image)[1], tf.shape(target_image)[2]])\n",
    "\n",
    "            up1 = slim.conv2d_transpose(icnv2, 16, [3, 3], stride=2, scope='upcnv1')\n",
    "            i1_in = tf.concat([up1, disp2_up], axis=3)\n",
    "            icnv1 = slim.conv2d(i1_in, 16, [3, 3], stride=1, scope='icnv1')\n",
    "            disp1 = (slim.conv2d(icnv1, 1, [3, 3], stride=1, activation_fn=tf.sigmoid,\n",
    "                               normalizer_fn = None, scope='disp1')\n",
    "                   * DISP_SCALING + MIN_DISP)\n",
    "\n",
    "            return [disp1, disp2, disp3, disp4]\n",
    "\n",
    "def egomotion_net(image_stack, is_training =True):\n",
    "    # Predict ego-motion vectors from a stack of frames.\n",
    "    #    Network inputs will be [1, 2, 3]\n",
    "    #    Network outputs will be [1 -> 2, 2 -> 3]\n",
    "    # Returns:\n",
    "    #    Egomotion vectors with shape [B, seq_length - 1, 6].    \n",
    "    batch_norm_params = {'is_training': is_training}\n",
    "    num_egomotion_vecs = seq_length - 1\n",
    "    with tf.variable_scope('pose_exp_net') as sc:\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        normalizer_fn = slim.batch_norm if FLAGS.use_bn else None\n",
    "        normalizer_params = batch_norm_params if FLAGS.use_bn else None\n",
    "        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n",
    "                            normalizer_fn=normalizer_fn,\n",
    "                            weights_regularizer=slim.l2_regularizer(WEIGHT_REG),\n",
    "                            normalizer_params=normalizer_params,\n",
    "                            activation_fn=tf.nn.relu):\n",
    "            cnv1 = slim.conv2d(image_stack, 16, [7, 7], stride=2, scope='cnv1')\n",
    "            cnv2 = slim.conv2d(cnv1, 32, [5, 5], stride=2, scope='cnv2')\n",
    "            cnv3 = slim.conv2d(cnv2, 64, [3, 3], stride=2, scope='cnv3')\n",
    "            cnv4 = slim.conv2d(cnv3, 128, [3, 3], stride=2, scope='cnv4')\n",
    "            cnv5 = slim.conv2d(cnv4, 256, [3, 3], stride=2, scope='cnv5')\n",
    "\n",
    "          # Ego-motion specific layers\n",
    "            with tf.variable_scope('pose'):\n",
    "                cnv6 = slim.conv2d(cnv5, 256, [3, 3], stride=2, scope='cnv6')\n",
    "                cnv7 = slim.conv2d(cnv6, 256, [3, 3], stride=2, scope='cnv7')\n",
    "                pred_channels = EGOMOTION_VEC_SIZE * num_egomotion_vecs\n",
    "                egomotion_pred = slim.conv2d(cnv7,\n",
    "                                             pred_channels,\n",
    "                                             [1, 1],\n",
    "                                             scope='pred',\n",
    "                                             stride=1,\n",
    "                                             normalizer_fn=None,\n",
    "                                             activation_fn=None)\n",
    "                egomotion_avg = tf.reduce_mean(egomotion_pred, [1, 2])\n",
    "                # Tinghui found that scaling by a small constant facilitates training.\n",
    "                egomotion_final = 0.01 * tf.reshape(\n",
    "                egomotion_avg, [-1, num_egomotion_vecs, EGOMOTION_VEC_SIZE])\n",
    "\n",
    "                return egomotion_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
